# 一致性

>希望数据是安全的，因此数据不能存在单点上。分布式系统对fault tolorence(容错)的一般解决方案是state machine replication(状态机复制)/数据库初始为0，赋值为5，那么a=5这个函数或日志称为log/state machine，数据库通过保存日志来记录操作，将这一连串log复制到其他节点上。
>
>paxos其实是一个共识consensus算法。系统的最终一致性不仅需要达成共识，还会取决于client的行为。

### 分布式系统的三个特性：

> CAP Theorem对于一个分布式系统，不能同时满足以下三点

1. #### 一致性 Consistency

     每个服务的操作会获得同样的结果。

​    

2. #### 可用性 Avaliability

    服务是可用的

    

3. #### 分区容错性Partition Tolerance

    如果两个地区存在服务，并且两个服务之间失去连接，那么认为这两个地区的服务是分区的。分区容错性是指在这种情况下的表现方式。在实际情况下，会在一致性和可用性之间进行取舍。



### 一致性模型

#### 弱一致性

+ 最终一致性：对数据库进行写入的同时，对数据库进行读取，不会读到这词写入，但最终会读到这次写入。
    + DNS	Domain Name System	域名系统
    + Gossip Cassandra的通信协议

#### 强一致性/重点

+ 同步
+ Paxos
+ Raft(multi-paxos)
+ ZAB(multi-paxos)

### 强一致性算法

#### 主从同步

1. master接受写请求
2. master复制日志到slave
3. master等到，直到所有从库返回

问题：一个节点失败，master阻塞，导致整个集群不可用，保证了一致性，可用性却大大降低。

#### 多数派

每次写都保证写入大于N/2个节点，每次读保证从大于N/2个节点中读。多数派读和多数派写。

问题：在并发环境下，无法保证系统正确性。顺序非常重要

#### Paxos/重点

+ basic paxos

    + 角色：

    ​	Client：系统外部角色，请求发起者/民众

    ​	Proposer：接受Client请求，向集群提出提议propose。并在冲突发生时，起到冲突调节的作用。/议员

    ​	Accpetor(Voter)：提议投票和接受者，只有在形成法定人数（Quorum，一般为majority多数派时），提议才会最终被接受。/国会

    ​	Learner：提议接受者，backup，备份，对集群一致性没什么影响。/记录员

    + 阶段：

        成功或部分失败打倒了quoroms

        Client请求-Proposer请求-Accpetor返回同意/多数同意-Proposer发出accept命令-acceptor返回acceoted结果同时Learner记录-Learner返回记录结果给Client

        proposer失败

        Client请求-Proposer请求-Accpetor返回同意/多数同意-Proposer宕机无法请求-生成新的proposer，此时带着新的**zxid发出accept命令**-acceptor返回acceoted结果同时Learner记录-Learner返回记录结果给Client

        潜在问题：活锁/zxid解决，难实现，效率低(两轮rpc)

+ multi paxos

    + Leader：唯一的proposer 

+ fast paxos

#### raft

+ 划分为三个子问题
    + leader election/选leader/timeout
    + log replication/同步log/在心跳包中，写入日志-commit
    + safety/所有操作的一致性/失败：quorum，分区：quorum-zxid
+ 重定义角色
    + leader
    + follower
    + candidate(LOOKING状态)

raft的git模拟： Raft.github.io

问题：如果client发出请求，五个节点中的三个宕机，两个可用，会出现两个可用节点写入并不提交，因为提交操作是需要先判断是否quorum达到多数派的。这时client会返回结果timeout/unknown，在最后机器恢复时，可能成功或者失败(被覆盖)。但是整个系统的共识还是没有问题的。

#### zab

与raft类似，zab将一个leader的周期称为epoch，而raft称之为term。raft保证日志连续性，心跳方向是leader到follower。zab则相反。

##### 1. Leader election（选举阶段）：

​	协议并没有规定详细的选举算法，后面我们会提到实现中使用的 Fast Leader Election。

##### 2. Discovery (发现)

发现阶段，用于在从节点中发现最新的ZXID和事务日志。或许有人会问：既然Leader被选为主节点，已经是集群里数据最新的了，为什么还要从节点中寻找最新事务呢？

这是为了防止某些意外情况，比如因网络原因在上一阶段产生多个Leader的情况。

所以这一阶段，Leader集思广益，接收所有Follower发来各自的最新epoch值。Leader从中选出最大的epoch，基于此值加1，生成新的epoch分发给各个Follower。

各个Follower收到全新的epoch后，返回ACK给Leader，带上各自最大的ZXID和历史事务日志。Leader选出最大的ZXID，并更新自身历史日志。

##### 4. **Broadcast**	单调一致性

1.客户端发出写入数据请求给任意Follower。

2.Follower把写入数据请求转发给Leader。

3.Leader采用二阶段提交方式，先发送Propose广播给Follower。

4.Follower接到Propose消息，写入日志成功后，返回ACK消息给Leader。

5.Leader接到半数以上ACK消息，返回成功给客户端，并且广播Commit请求给Follower。



##### 协议实现

协议的 Java 版本实现跟上面的定义有些不同，选举阶段使用的是 Fast Leader Election（FLE），它包含了 Phase 1 的发现职责。因为 FLE 会选举拥有最新提议历史的节点作为 leader，这样就省去了发现最新提议的步骤。实际的实现将 Phase 1 和 Phase 2 合并为 Recovery Phase（恢复阶段）。所以，ZAB 的实现只有三个阶段：

- **Fast Leader Election**
- **Recovery Phase**
- **Broadcast Phase**

<img src="/Users/cty/Library/Application Support/typora-user-images/image-20201105134349887.png" alt="image-20201105134349887" style="zoom:25%;" />



###### 深入浅出Zookeeper之六 Leader/Follower初始化-java演示

###### https://www.iteye.com/blog/iwinit-1775439

大牛分析zab

https://juejin.im/post/6844903672128733198